{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-26T17:35:45.482078Z","iopub.execute_input":"2022-06-26T17:35:45.482564Z","iopub.status.idle":"2022-06-26T17:35:51.419224Z","shell.execute_reply.started":"2022-06-26T17:35:45.482465Z","shell.execute_reply":"2022-06-26T17:35:51.417864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install einops","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:35:51.421622Z","iopub.execute_input":"2022-06-26T17:35:51.422092Z","iopub.status.idle":"2022-06-26T17:36:04.561672Z","shell.execute_reply.started":"2022-06-26T17:35:51.422034Z","shell.execute_reply":"2022-06-26T17:36:04.560376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom typing import List, Union\nfrom pathlib import Path\nfrom torch.utils.data import random_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport time\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torch.optim import Adam\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:04.563563Z","iopub.execute_input":"2022-06-26T17:36:04.563965Z","iopub.status.idle":"2022-06-26T17:36:07.233618Z","shell.execute_reply.started":"2022-06-26T17:36:04.56393Z","shell.execute_reply":"2022-06-26T17:36:07.2323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = datasets.ImageFolder('/kaggle/input/iais22-birds/birds/birds/', transform=transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()]))\ntrain_set, val_set = random_split(dataset, (int(len(dataset) * 0.9) + 1, int(len(dataset) * 0.1)))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:07.236462Z","iopub.execute_input":"2022-06-26T17:36:07.237106Z","iopub.status.idle":"2022-06-26T17:36:07.763717Z","shell.execute_reply.started":"2022-06-26T17:36:07.237071Z","shell.execute_reply":"2022-06-26T17:36:07.762401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = DataLoader(dataset, batch_size=200, shuffle=True)\ntrain_dataloader = DataLoader(train_set, batch_size=200, shuffle=True)\nval_dataloader = DataLoader(val_set, batch_size=200, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:07.765583Z","iopub.execute_input":"2022-06-26T17:36:07.766051Z","iopub.status.idle":"2022-06-26T17:36:07.773057Z","shell.execute_reply.started":"2022-06-26T17:36:07.766003Z","shell.execute_reply":"2022-06-26T17:36:07.77185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_dataloader = DataLoader(dataset, batch_size=200)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:07.774965Z","iopub.execute_input":"2022-06-26T17:36:07.775719Z","iopub.status.idle":"2022-06-26T17:36:07.789863Z","shell.execute_reply.started":"2022-06-26T17:36:07.775667Z","shell.execute_reply":"2022-06-26T17:36:07.788767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pair(t):\n    return t if isinstance(t, tuple) else (t, t)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:07.791152Z","iopub.execute_input":"2022-06-26T17:36:07.791708Z","iopub.status.idle":"2022-06-26T17:36:07.802311Z","shell.execute_reply.started":"2022-06-26T17:36:07.791675Z","shell.execute_reply":"2022-06-26T17:36:07.801069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Encoder(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, MLP(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        #Obtiene la altura y el ancho de la imagen\n        image_height, image_width = pair(image_size)\n        #Obtiene la altura y el ancho del patch\n        patch_height, patch_width = pair(patch_size)\n        \n        #Comprueba que la altura de la imagen sea divisible por la altura del patch y lo mismo con el ancho\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n        \n        #Obtiene el numero de veces en el que se ha dividido la imagen\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        #Las dimensiones de cad subtrozo de imagen\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n        \n        #Capa EMBEDDED PATCHES\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n            nn.Linear(patch_dim, dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        #ENCODER\n        self.encoder = Encoder(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n        \n        #MLP HEAD\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        #EMBEDDED PATCHES\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n        cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n        \n        #Se aplica el ENCODER\n        x = self.encoder(x)\n        \n        #Si se ha declarado que se use la media se usa, si no, se toma la primera columna\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n        \n        #Función de activación la identidad\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:07.803996Z","iopub.execute_input":"2022-06-26T17:36:07.804605Z","iopub.status.idle":"2022-06-26T17:36:07.837399Z","shell.execute_reply.started":"2022-06-26T17:36:07.804561Z","shell.execute_reply":"2022-06-26T17:36:07.836356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T17:36:07.838951Z","iopub.execute_input":"2022-06-26T17:36:07.839517Z","iopub.status.idle":"2022-06-26T17:36:07.855618Z","shell.execute_reply.started":"2022-06-26T17:36:07.839482Z","shell.execute_reply":"2022-06-26T17:36:07.854313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ViT(\n    image_size = 64,\n    patch_size = 8,\n    num_classes = 400,\n    dim = 256,\n    depth = 2,\n    heads = 12,\n    mlp_dim = 512,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T19:32:37.985763Z","iopub.execute_input":"2022-06-26T19:32:37.986688Z","iopub.status.idle":"2022-06-26T19:32:38.015512Z","shell.execute_reply.started":"2022-06-26T19:32:37.986644Z","shell.execute_reply":"2022-06-26T19:32:38.014255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-3\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = Adam(params=model.parameters(), lr=learning_rate, amsgrad=False)\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    \n    \n\n\nepochs = 15\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train, model, loss_fn, optimizer)\n    test_loop(val_dataloader, model, loss_fn)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T19:32:38.754652Z","iopub.execute_input":"2022-06-26T19:32:38.755085Z","iopub.status.idle":"2022-06-26T23:35:38.60176Z","shell.execute_reply.started":"2022-06-26T19:32:38.755051Z","shell.execute_reply":"2022-06-26T23:35:38.597354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = './transformer-15epo-12h.pth'\ntorch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:35:46.040303Z","iopub.execute_input":"2022-06-26T23:35:46.041081Z","iopub.status.idle":"2022-06-26T23:35:46.070832Z","shell.execute_reply.started":"2022-06-26T23:35:46.041037Z","shell.execute_reply":"2022-06-26T23:35:46.069986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"red = ViT(\n    image_size = 64,\n    patch_size = 8,\n    num_classes = 400,\n    dim = 256,\n    depth = 2,\n    heads = 12,\n    mlp_dim = 512,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\nred.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:35:51.087041Z","iopub.execute_input":"2022-06-26T23:35:51.087502Z","iopub.status.idle":"2022-06-26T23:35:51.143482Z","shell.execute_reply.started":"2022-06-26T23:35:51.087464Z","shell.execute_reply":"2022-06-26T23:35:51.142499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nclass BirdsDatasetTest(torch.utils.data.Dataset):\n    def __init__(self, path: Union[Path, str],\n                transform: Union['Transform', List['Transform']] = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])):\n        self.path = Path(path)\n        self.labels = [p.name for p in path.glob('x')]\n        self.images = list(path.glob('*/*.jpg'))\n        self.transform = transform\n        \n    \n    def __len__(self) -> int:\n        return len(self.images)\n    \n    def __getitem__(self, index:int) :\n        image_path = self.images[index]\n        image = self.transform(Image.open(str(image_path)))\n        archivo = os.path.basename(image_path)\n        id = archivo.split(sep=\".\")[0]\n        return image,id","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:35:52.783217Z","iopub.execute_input":"2022-06-26T23:35:52.783839Z","iopub.status.idle":"2022-06-26T23:35:52.795535Z","shell.execute_reply.started":"2022-06-26T23:35:52.783787Z","shell.execute_reply":"2022-06-26T23:35:52.794586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dat = BirdsDatasetTest(path = Path('/kaggle/input/iais22-birds/submission_test/'))\nclases = dataset.classes\ndataloader = DataLoader(test_dat, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:35:54.663494Z","iopub.execute_input":"2022-06-26T23:35:54.664115Z","iopub.status.idle":"2022-06-26T23:35:55.196555Z","shell.execute_reply.started":"2022-06-26T23:35:54.664072Z","shell.execute_reply":"2022-06-26T23:35:55.195265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport os\ntrans = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\nwith open('submission.csv', 'w') as csvfile:\n    fieldnames = ['Id', 'Category']\n    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n    writer.writeheader()\n    for image,id in dataloader:\n        pred = red(image)\n        #predicted = torch.argmax(pred, 1)\n        #print(int(id[0]))\n        categoria = clases[pred.argmax(1)]\n        writer.writerow({'Id': int(id[0]), 'Category': categoria})","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:35:56.262064Z","iopub.execute_input":"2022-06-26T23:35:56.262459Z","iopub.status.idle":"2022-06-26T23:36:19.973068Z","shell.execute_reply.started":"2022-06-26T23:35:56.262426Z","shell.execute_reply":"2022-06-26T23:36:19.971817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}